# 特征选择

## 一、Lasso回归
lasso回归和岭回归分别是线性回归加上L1和L2正则项。
L1正则项为
$$ \lambda \sum_{j=1}^n |W_{j}|$$
L2正则项为
$$\lambda\sum_{j=1}^n W_j^2 $$

岭回归不能用作特征选择，lasso回归在L1正则项和目标函数一般相交于
### 正则项


## 二、pearson相关系数
$$ \rho_{X,Y}=\frac{cov(X,Y)}{\sigma_X\sigma_Y} = \frac{E((X-\mu_X)(Y-\mu_Y))}{\sqrt{E((X-\mu_X)^2)}\sqrt{E((Y-\mu_Y)^2)}}$$
相关系数的绝对值越大，相关性越强

## 三、随机森林
### 3.1 随机森林构建步骤如下

1. 原始数据集S，此时树的深度depth=0。
2. 针对集合S，遍历每一个feature的每一个value，用该value将原数据集S分裂成2个集合：左集合S_left(<=value的样本)、右集合S_right(>value的样本)，每一个集合也叫做一个结点。分别计算这2个集合的mse，找到使得（left_mse+right_mse）最小的那个value，记录下此时的feature名称和value，这个就是最佳分割特征以及最佳分割值；
每一个集合/结点mse的计算方法如下：

   * mean = sum(该集合内每一个样本的目标值) / 该集合内样本总数。（ps：这个mean就是该结点的值，也就是落在该结点内的样本的预测值，同一个结点中的样本具有同一个预测值。）
   * mse = sum((该集合内每一个样本的目标值 - mean)^2 )

    为什么要用均方差mse来作为分裂的依据呢？  
    只要是能衡量预测值和真实值/目标值之间的差距的数学公式，都可以用，例如信息增益、信息增益比、基尼系数等等。但是均方差有更好的好处：一阶导数和二阶导数可求并好求。

3. 找到最佳分割feature以及最佳分割value之后，用该value将集合S分裂成2个集合：左集合S_left、右集合S_right，每一个集合也叫做一个结点。此时树的深度depth += 1。
4. 针对集合S_left、S_right分别重复步骤2,3，直到达到终止条件。

    终止条件有：  
1、特征已经用完了：没有可供使用的特征再进行分裂了，则树停止分裂；  
2、子结点中的样本已经都是同一类：此时，样本已经全部被划分出来了，不用再进行区分，该结点停止分裂（不过一般很难达到，达到的话，该树肯定过拟合）；  
3、子节点中没有样本了：此时该结点已经没有样本可供划分，该结点停止分裂；
很多算法中还有以下终止条件，为了防止过拟合：
    1、树达到了最大深度：depth >= max_depth，树停止分裂。  
    2、结点的样本数量达到了阈值：如果一个集合（结点）的样本数量 < min_samples_leaf，则树停止分裂；  
    其中，max_depth和min_samples_leaf都是人为制定的超参数。