# 贝叶斯神经网络

## 概述

### 贝叶斯公式
$$p(W|X,Y)=(\frac{p(W)p(Y|X,W)}{p(Y|X)})$$  
$p(Y|X)$即为数据集，根据后验概率公式，$p(W|X,Y)\propto p(W)p(Y|X,W)$，其中$p(W)$为先验概率，即已知的网络参数，$p(Y|X,W)$为在当前网络参数下，对于输入样本X可以得到Y的概率。

由于网络拟合模型复杂，无法求得均值与方差，贝叶斯神经网络求得了一个q函数来逼近p函数，如利用简单高斯分布求得均值$\mu_w$和方差$\sigma_w$来逼近p函数，其中的均值$\mu_w$和方差$\sigma_w$就是神经网络要训练的更新的参数。优化方向即是使KL散度最小。

### KL散度

熵公式是  
$$H=-\sum_{i=1}^N p(x_i)\log p(x_i)$$  

KL散度是判断两种分布的逼近程度的，称为两种分布的相对熵
$$KL(q||p)=\sum_{i=1}^n q(W)\log\frac{q(W)}{p(W|X,Y)}$$
它实质上是一个关于q的期望
$$KL(q||p)=E_q[\log\frac{q(W)}{p(W|X,Y)}]$$
降低KL散度的操作称为“变分”。
整理得
$$KL(q||p)=E_q[\log q(W)-\log p(W)-\log p(Y|X,W)+\log p(Y|X)]$$
其中$p(Y|X)$为常数（是已知的）
简化为
$$Loss=E_q[\log q(W)-\log p(W)-\log p(Y|X,W)]$$

$\log q(W)$这一项很好计算  是先验概率，一般我们指定为均值为0方差为的高斯分布即可；在上文中提到,  因此对 累加求和取平均值即可求出$\log p(W)$；$\log p(Y|X,W)$的计算十分关键，这一项是表示在给定参数和自变量的情况下，输出$y_{pred}$为数据集中所对应的$y$的概率，采用如下公式来计算：$p(Y|X,W)=\frac {1}{\sqrt{2\pi}\sigma}e^{((Y-Y_pred))}$, 其中  为Y 的先验参数（的先验方差一般与的先验方差相同）。

## 优点
贝叶斯神经网络的优点是可以根据较少的数据得到较为solid的模型。可以有效的解决过拟合的问题，不仅可以对结果进行预测，还可以对结果的误差进行有效预测

